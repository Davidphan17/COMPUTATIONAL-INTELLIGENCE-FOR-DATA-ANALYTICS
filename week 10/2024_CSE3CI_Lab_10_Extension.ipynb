{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d708eed8",
   "metadata": {},
   "source": [
    "# 2022_CSE3CI_Lab_10_Extension LSTM for Text Generation\n",
    "In this exercise, we are going to use a popular book from childhood as the dataset: Alice’s Adventures in Wonderland by Lewis Carroll. The dataset is downloaded from the online free book sources: https://www.gutenberg.org/ and named as wonderland.txt. After this practice, you can also download other free books and try build network and make predictions yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a268bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1460ee67",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "250ec122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92443cd",
   "metadata": {},
   "source": [
    "### Prepare data for RNN\n",
    "Note: we cannot model the characters directly, instead we must convert the characters to integers.\n",
    "This can be done easily by first creating a set of all of the distinct characters in the book, then creating a map of each character to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f673b6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96fc045c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '\"': 3,\n",
       " '#': 4,\n",
       " '$': 5,\n",
       " '%': 6,\n",
       " \"'\": 7,\n",
       " '(': 8,\n",
       " ')': 9,\n",
       " '*': 10,\n",
       " ',': 11,\n",
       " '-': 12,\n",
       " '.': 13,\n",
       " '/': 14,\n",
       " '0': 15,\n",
       " '1': 16,\n",
       " '2': 17,\n",
       " '3': 18,\n",
       " '4': 19,\n",
       " '5': 20,\n",
       " '6': 21,\n",
       " '7': 22,\n",
       " '8': 23,\n",
       " '9': 24,\n",
       " ':': 25,\n",
       " ';': 26,\n",
       " '?': 27,\n",
       " '[': 28,\n",
       " ']': 29,\n",
       " '_': 30,\n",
       " 'a': 31,\n",
       " 'b': 32,\n",
       " 'c': 33,\n",
       " 'd': 34,\n",
       " 'e': 35,\n",
       " 'f': 36,\n",
       " 'g': 37,\n",
       " 'h': 38,\n",
       " 'i': 39,\n",
       " 'j': 40,\n",
       " 'k': 41,\n",
       " 'l': 42,\n",
       " 'm': 43,\n",
       " 'n': 44,\n",
       " 'o': 45,\n",
       " 'p': 46,\n",
       " 'q': 47,\n",
       " 'r': 48,\n",
       " 's': 49,\n",
       " 't': 50,\n",
       " 'u': 51,\n",
       " 'v': 52,\n",
       " 'w': 53,\n",
       " 'x': 54,\n",
       " 'y': 55,\n",
       " 'z': 56,\n",
       " 'ù': 57,\n",
       " '—': 58,\n",
       " '‘': 59,\n",
       " '’': 60,\n",
       " '“': 61,\n",
       " '”': 62}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2276cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  164045\n",
      "Total Vocab:  63\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1dd660",
   "metadata": {},
   "source": [
    "Then, we will split the book text up into subsequences with a fixed length of 100 characters, an arbitrary length. Another way is to split the data up by sentences and pad the shorter sequences and truncate the longer ones.\n",
    "\n",
    "\n",
    "Each training pattern of the network is comprised of 100 time steps of one character (X) followed by one character output (y). When creating these sequences, we slide this window along the whole book one character at a time, allowing each character a chance to be learned from the 100 characters that preceded it (except the first 100 characters of course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5afb46b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  163945\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eef054",
   "metadata": {},
   "source": [
    "Next we need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the RNN network. Then, we need to convert the output patterns (single characters converted to integers) into a one hot encoding. This is so that we can configure the network to predict the probability of each of the 47 different characters in the vocabulary (an easier representation) rather than trying to force it to predict precisely the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3ffff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c88686bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163945, 100, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b5ebe1",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "You can either train the model yourself (take some time) or load the weights that I have provided on LMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ca226f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\py37tf\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\py37tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/20\n",
      "163945/163945 [==============================] - 465s 3ms/step - loss: 3.0347\n",
      "Epoch 2/20\n",
      "163945/163945 [==============================] - 464s 3ms/step - loss: 2.8629\n",
      "Epoch 3/20\n",
      "163945/163945 [==============================] - 464s 3ms/step - loss: 2.7810\n",
      "Epoch 4/20\n",
      "163945/163945 [==============================] - 462s 3ms/step - loss: 2.7102\n",
      "Epoch 5/20\n",
      "163945/163945 [==============================] - 462s 3ms/step - loss: 2.6539\n",
      "Epoch 6/20\n",
      "163945/163945 [==============================] - 462s 3ms/step - loss: 2.5986\n",
      "Epoch 7/20\n",
      "163945/163945 [==============================] - 464s 3ms/step - loss: 2.5461\n",
      "Epoch 8/20\n",
      "163945/163945 [==============================] - 465s 3ms/step - loss: 2.4988\n",
      "Epoch 9/20\n",
      "163945/163945 [==============================] - 466s 3ms/step - loss: 2.4539\n",
      "Epoch 10/20\n",
      "163945/163945 [==============================] - 467s 3ms/step - loss: 2.4142\n",
      "Epoch 11/20\n",
      "163945/163945 [==============================] - 467s 3ms/step - loss: 2.3744\n",
      "Epoch 12/20\n",
      "163945/163945 [==============================] - 466s 3ms/step - loss: 2.3397\n",
      "Epoch 13/20\n",
      "163945/163945 [==============================] - 467s 3ms/step - loss: 2.3051\n",
      "Epoch 14/20\n",
      "163945/163945 [==============================] - 465s 3ms/step - loss: 2.2748\n",
      "Epoch 15/20\n",
      "163945/163945 [==============================] - 465s 3ms/step - loss: 2.2417\n",
      "Epoch 16/20\n",
      "163945/163945 [==============================] - 464s 3ms/step - loss: 2.2142\n",
      "Epoch 17/20\n",
      "163945/163945 [==============================] - 457s 3ms/step - loss: 2.1995\n",
      "Epoch 18/20\n",
      "163945/163945 [==============================] - 456s 3ms/step - loss: 2.1608\n",
      "Epoch 19/20\n",
      "163945/163945 [==============================] - 457s 3ms/step - loss: 2.1428\n",
      "Epoch 20/20\n",
      "163945/163945 [==============================] - 458s 3ms/step - loss: 2.1234\n"
     ]
    }
   ],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit(X, y, epochs=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d108eba",
   "metadata": {},
   "source": [
    "### Generate text with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcbb29d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start from\n",
      "alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to\n",
      " the wirl an the was oo the wooee \n",
      "the woile the was oo the wooed th the war oo the was of the courd  the was oo the wored the was ano a froa tiie the was oo the wooe, and the wer toee a aot oo the woold \n",
      "the had not aelen a lottle oo the tas of the coure, \n",
      "“he you dan nete the world ”ou hake ien toae i sooel saad the morse  ih wou dan nete t aaice be anl afnin ao iers \n",
      "“hh you dan nete toe thing ”ou hake ”ou kake i tooel ii wou dan nete t aaice be an innio to the tiiel of the wools. and the wes to aeain to tee thete sas in the dourd aedin, and the west on thet whre the wooed th the war oo the wooe, \n",
      "“he you dan nete the world toan ai a moadt ”asle the woils!” said the mock turtle.\n",
      "\n",
      "“teal t shoukd the marter wan i sail to the woils ”fnl would ”hu  a dit dan t aad to tee thile was soe tai in the courd, \n",
      "“he you dan nete the gorst seat ”hu ” said the mock turtle \n",
      "tie was oo the was oo the wored the was anoie the was oo the wooe, \n",
      "“he you dan nete the world toan ai a moadt ”asle the woils\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a start of text to make predictions\n",
    "start = 1468\n",
    "pattern = dataX[start]\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "print(\"start from\")\n",
    "print(''.join([int_to_char[value] for value in pattern]))\n",
    "\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c65e6d",
   "metadata": {},
   "source": [
    "You can try build the following more complex LSTM model yourself and see if any change on the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c2964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128)\n",
    "\n",
    "model.save_weights('lstm_weights.h5')\n",
    "\n",
    "# you can pick a random seed or give a specific start as we do before\n",
    "# start = numpy.random.randint(0, len(dataX)-1)\n",
    "start = 1468\n",
    "pattern = dataX[start]\n",
    "print(\"start from\")\n",
    "print(''.join([int_to_char[value] for value in pattern]))\n",
    "\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
